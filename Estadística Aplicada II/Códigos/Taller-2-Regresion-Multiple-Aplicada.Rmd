---
title: "Taller 2 Regresión lineal Multiple"
author: "Andrés Felipe Palomino - David Stiven Rojas"
date: "2023-04-21"
output: 
  pdf_document: 
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo =TRUE)
```

# Introducción

La base de datos $"yarn"$ obtenida de la librería (PLS) contiene información sobre espectros NIR y mediciones de densidad de hilos de PET, consta de 28 individuos (hilos de PET), 268 variables predictoras (NIRS) y una variable de respuesta (densidad). Se ajustará un modelo lineal múltiple para estimar la densidad del hilo PET, mediante mediciones NIR

```{r, echo = T, results = 'asis', message= FALSE, warning= FALSE}
#Importación de librerías necesarias
library(car)
library(glmnet)
library(MASS)
library(xtable)
library(lmtest)
library(readxl)
library(lmridge)
library(pls)
library(olsrr)
library(effects)
```

## Base de datos

En la siguiente tabla se encuentra un encabezado de la base de datos que se trabajara, esta consta de 30 covariables predictoras, las cuales estarán desde NIR1 hasta NIR30. De primera mano se observa que los valores de los NIR disminuyen a medida que la covariable aumenta

```{r, echo = T, results = 'asis', message= FALSE}

X <- data.frame(matrix(c(yarn$NIR[,1:30],yarn$density),nrow =28, ncol= 31))
colnames(X) <- c(paste("NIR",1:30,sep=""),"density")



```

## Funciones creadas

Antes de empezar con el proceso de seleccionar las variables para ajustar el modelo se crean funciones para optimizar el proceso de validación de supuestos, debido a que constantemente se deben realizar, estas funciones estan diseñadas para objetos lm.

```{r,echo = T, results = 'asis', message= FALSE}
##Validacion grafica para homocedasticidad y normalidad y pruebas formales
validaciongrafica<- function(model,cor=F){
  
  par(mfrow=c(1,2))
  plot(fitted.values(model),studres(model),panel.first=grid(),
       pch=19,ylab='Residuos Estudentizados',xlab='Valores ajustados',main='A',col='aquamarine4')
  abline(h=c(-2,0,2),lty=2)
  qqPlot(model,pch=19,ylab='Residuos Estudentizados',
         xlab='Cuantiles Teóricos',col=carPalette()[1],
         col.lines=carPalette()[3],main='B')
  print('Shapiro Test; H0: Normalidad vs H1: No Normalidad')
  print(shapiro.test(studres(model)))
  print('Breusch Pagan Test;H0: Homocedasticidad vs H1: No Homocedasticidad')
  print(bptest(model))
  if(cor==T){
    par(mfrow=c(1,2))
    plot(studres(model),type="b",xlab="Tiempo",ylab="Residuos Estudentizados",main="A",
         pch=19,panel.first=grid())
    plot(studres(model)[-length(fitted.values(model))],
         studres(model)[-1],pch=19,panel.first = grid(),col="turquoise3",
         xlab=TeX("$Residuos_{t-1}$"),ylab=TeX("$Residuos_{t}$"),main="B")
    abline(lm(studres(model)[-1]~studres(model)[-length(fitted.values(model))]))
    print('Durbin Watson Test')
    print(durbinWatsonTest(model,
                           method='resample',reps=10000))
  }
  par(mfrow=c(1,1))
}

## Calculo de lambda optimo para boxcox
lambda<- function(model,a,b){
  par(mfrow=c(1,1))
  box.cox<-boxcox(model,lambda=seq(a,b,length.out = 1000),
                  ylab='log-verosimilitud')
  bc<-round(box.cox$x[box.cox$y ==max(box.cox$y)],2)
  print(bc)
}

```

# Selección de variables

En el proceso de selección de variables se procede a realizar la Regresion de LASSO para identificar las posibles variables que tengan un aporte poco relevante, Por ultimo se ajustara el modelo cuyas variables tengan buenos indicadores y se pueda realizar corrección de supuestos

## Regresión de LASSO

Este es un método de regularización que se implementa cuando se tiene muchas covariables disponibles y se cree que pocas tienen un aporte relevante.

Se asume el modelo de regresión usual, donde :

```{=tex}
\begin{center}
E(y|x)=$X^T\beta$, y V(y|x)=$\sigma^2$
\end{center}
```
Donde se asume que algunos $\beta$ son cero. El objetivo del estimador es seleccionar los coeficientes que tienen valores diferentes de cero. El cual se obtiene minimizando la siguiente expresión:

```{=tex}
\begin{center}
$S_{lasso}(\beta)=\sum_{i=1}^{n}({y_{i}-x^
T}\beta)^2+\lambda\sum_{j=1}^{p-1}|\beta_{j}|$
\end{center}
```
Esta es la suma de cuadrados del estimador por MCO más una penalización ($\lambda$), a la suma del valor absoluto de los coeficientes. A medida que $\lambda$ aumenta la penalización tendrá mas peso sobre la estimación de los coeficientes, es decir que si la penalización es muy grande, todas las estimaciones seran cero. No hay solución analitica para $\hat{\beta}_{lasso}$ por lo que se usan algoritmos para la estimación, como lo es la funcion de glmnet de la libreria glmnet.

### Modelo a realizar regresión LASSO

Como se establecio anteriormente, se asume un modelo de regresión usual, el cual debe cumplir los siguientes supuestos: E(y\|x)=$x^T\beta$, y V(y\|x)=$\sigma^2$, es decir, varianza constante y E($\varepsilon$)=0 . Por ende es necesario proponer un modelo con p\<n, en el cual se eliminaran las variables con menor correlación con la variable density. Dicho modelo se expresa acontinuación y se evaluan los supuestos:

```{r, echo = T, message= FALSE}

model <- lm(density ~ .-NIR1-NIR8-NIR9-NIR10-NIR11-NIR7, data=X)
car::vif(model)[1:5]
car::vif(model)[6:10]
car::vif(model)[11:16]
car::vif(model)[17:24]
validaciongrafica(model)
```

Mediante el grafico y los valores P asociados a la homocedasticidad y normalidad de residuos se evidencia el incumplimiento de la normalidad de los residuos.

```{r, echo = T, message= FALSE}
hist(studres(model),lwd=2,col='aquamarine4',freq=F,ylim=c(0,0.4),
     xlab='Residuos Estudentizados Modelo',main='')
lines(density(studres(model)),lwd=2,col='black')

```

Como no se cumple el supuesto de normalidad se procede a corregir mediante el metodo de BoxCox y se verifica el cumplimiento de los mismos.

```{r, echo = T, message= FALSE, fig.width=4, fig.height=3}
model <- lm(density+0.01 ~ .-NIR1-NIR8-NIR9-NIR10-NIR11-NIR7, data=X)
lambda(model,0.5,1.5)
```

```{r, echo = T, results = 'asis', message= FALSE}
model.box <- lm(I(density^0.96) ~.-NIR1-NIR8-NIR9-NIR10-NIR11-NIR7,data=X)
validaciongrafica(model.box)
```

Mediante el grafico y los valores P asociados a la homocedasticidad y normalidad de residuos se evidencia el cumplimiento de ambos supuestos.

```{r, echo = T, results = 'asis', message= FALSE}
hist(studres(model.box),lwd=2,col='aquamarine4',
freq=F,ylim=c(0,0.4),xlab='Residuos Estudentizados Modelo',main='')
lines(density(studres(model.box)),lwd=2,col='black')


```

Ya con los requerimentos necesarios para realizar regresión de LASSO se procede a calcular el valor de $\lambda$ optimo mediante la validación cruzada

### Validación cruzada

Es un método para evaluar que tan bueno es un modelo para predecir observaciones futuras de la población objeto de estudio. La muestra se divide en dos grupos:

-   Entrenamiento: Se usa para ajustar el modelo.

-   Validación: Se utiliza para validar el modelo ajustado.

Se realiza K interacciones dividiendo los datos en K subconjuntos. En cada interacción uno de los subconjuntos es utilizado para validación, el el resto (K-1) como datos de entrenamiento. Para cada división, k=1,..,k y para cada valor de $\lambda$ se estima el modelo basado en la muestra de entrenamiento. Mientras que con cada muestra de validación y para cada valor de $\lambda$ se utiliza para calcular el error cuadratico medio:

$$ECM_k(\lambda)= \sum_{i=1}^{n_k}\frac{(y_i^k- x_i^k*\hat{\beta}^k(\lambda))^2}{n}$$

Donde $y_i^k$ son las observaciones de la muestra de validación k y $\hat{\beta}^k$ es la estimacion utilizando la muestra de entrenamiento k. para cada $\lambda$ se calcula:

$CV(\lambda)=\frac{1}{k}\sum_{i=1}^{K}ECM_k(\lambda)$

Y la desviación estándar:

$SD(\lambda)=\sqrt{\sum_{i=1}^{K}\frac{(ECM_k(\lambda)-CV(\lambda))^2}{K-1}}$

$\lambda$ optimo: minimiza $CV{\lambda}$ o

maximiza $\lambda$: $CV(\hat{\lambda}) < CV(\hat{\lambda}_{cv})+SD(\hat{\lambda}_{cv})$

La maximisación es mas optima ya que se tiene en cuenta la variabilidad debida a la selección de las submuestras.

```{r, echo = T }
X.<-model.matrix(model.box)[,-1]
lasso.cv <-cv.glmnet(X., X$density, nfolds = 4, alpha = 1,
                     nlambda = 100)
plot(lasso.cv)
est = glmnet(X., X$density, alpha = 1,lambda = lasso.cv$lambda.1se)
est$beta

```

La selección de variables por medio del estimador LASSO son: NIR2, NIR6, NIR18, NIR28, NIR29. Consiguiente a eso se procede a realizar una prueba sobre subconjuntos para evaluar si podemos eliminar NIR29 para disminuir problemas de multicolinealidad.

## Suma extra de cuadrados

Sirve para probar la significancia de un subconjunto de coeficientes.

Se tiene el siguiente modelo:

$$y = X\beta+\varepsilon$$ donde $\beta$=$\begin{bmatrix} \beta_{1}\\ \beta_{2} \\ \end{bmatrix}$

donde $\beta_1$ es un vector (p-r)x1 y $\beta_2$ es un vector rx1 , se quiere evaluar la siguiente hipotesis:

```{=tex}
\begin{center}
$H_0 : \beta_2 =0$ vs $H_1: \beta_2 \neq 0 $
\end{center}
```
Se tienen los siguientes modelos: Modelo completo : $y = X_1\beta_1+X_2\beta_2+\varepsilon$

$SCR(B)= y^T(H-\frac{1}{n}11^T)y$

Modelo reducido : $y = X_1\beta_1\varepsilon$

$SCR(B_1)= y^T(H_1-\frac{1}{n}11^T)y$

La suma de cuadrados de la regresión debida a $\beta_2$ dado que $\beta_1$ ya esta en el modelo es: $$SSR(\beta_2|\beta_1)=SSR(\beta)-SSR(\beta_1)$$ Conocida como suma extra de cuadrados debido a $\beta_2$, y dado que queremos probar $H_0 : \beta_2 =0$ se construye el siguiente estadistico.

$F_0 = \frac{\frac{SSR(\beta_2|\beta_1)}{r}}{\frac{SE}{n-p}}$

Si $H_0$ es cierta entonces $F_0$ \~ $F_{r,n-p}$

Se realiza la respectiva prueba con la funcion anova, asumiento que el modelo reducido es aquel $\beta_5$=0 asociado al NIR29

Hipotesis: $H_0:\beta_5=0$ VS $H_1:\beta_5\neq0$

```{r, echo = T}
model.lasso1 <- lm(density~NIR2+NIR6+NIR18+NIR28+NIR29,data=X)
model.lasso2 <- lm(density~NIR2+NIR6+NIR18+NIR28,data=X)
anova(model.lasso2,model.lasso1)
```

La prueba anova indica un valor P de 0.4495 lo que indica que el coeficiente asociado al NIR29 es significativamente 0, por ende, podemos retirarlo del modelo ya que no aporta a la estimación de la densidad y en cambio aumenta el VIF como se evidencia acontinuación.

## Factor de inflación de varianza (VIF)

Es una medida que detecta si hay problemas de multicolinealidad. Generalmente un VIF mayor a 10 indica problemas graves de multicolinealidad.

$VIF_j = \frac{1}{1-R^2}$

Donde $R^2_j$ es el coeficiente de determinación obtenido ajustando una regresión de $x_j$ sobre las demás covariables.

$VIF_j = \sum_{j=1}^{p-1}\frac{t^2_{ij}}{\lambda_j}$

Donde $T=(t_1,....,t_{p-1})$ es una matriz ortogonal de vectores propios y $\lambda_j$ los valores propios asociados a la descomposición de la matriz $R=Z^TZ$ y Z es la matrix de X estandarizada. $R=TAT^T$ (descomposición)

```{r, echo = T}
car::vif(model.lasso1)
car::vif(model.lasso2)
```

# Modelo de regresión multiple

Con base en el proceso de selección de variables se ajusta el siguiente modelo y se realiza la respectiva validación de supuestos:

```{r, echo = T}
model.lasso1<- lm(density~NIR2+NIR6+NIR18+NIR28,data=X)
summary(model.lasso1)

```

## Interpretación

-   En el resumen del modelo contamos con un $R^2=0.9984$, es decir, el 99.84 de la variabilidad de la densidad del Hilo PET está siendo explicada por el modelo. Se cuenta con un $R^2_{adj}$ de 0.9981.

-   El valor del estadístico F es de 3584 con un valor p asociado de aproximadamente 0 lo que indica que por lo menos una de estas estimaciones de los $\beta_i$ es diferente de 0. Por ende ajustar el modelo ayuda a la predicción de la densidad del Hilo de Pet.

-   Observamos relaciones negativas entre la densidad y el conjunto de covariables NIR2, NIR 18 y NIR28 SI asumimos que se presentan aumentos con las demas covariables constantes, y se presenta una relación positiva entre la densidad y el NIR6 si mantenemos las demas covariables constantes.

#### ANOVA

```{r, echo = T}
Anova(model.lasso1)

```

La tabla ANOVA realiza pruebas sobre subconjuntos de coeficientes haciendo uso de la suma de cuadrados extra.

Estadistico F asociado al NIR2 : $F_0 = \frac{{SSR(\beta_1|\beta_0)}}{\frac{SE}{26}}$

Modelo: $y =\beta_0+NIR2\beta_1+\varepsilon$

Hipotesis: $H_0: \beta_1=0$

Estadistico F asociado al NIR6 : $F_0 = \frac{{SSR(\beta_2|\beta_1,\beta_0)}}{\frac{SE}{26}}$

Modelo: $y =\beta_0+NIR2\beta_1+NIR6\beta_2+\varepsilon$

Hipotesis: $H_0: \beta_2=0$

Estadistico F asociado al NIR18 : $F_0 = \frac{{SSR(\beta_3|\beta_0,\beta_1,\beta_2)}}{\frac{SE}{26}}$

Modelo: $y =\beta_0+NIR2\beta_1+NIR6\beta_2+NIR18\beta_3+\varepsilon$

Hipotesis: $H_0: \beta_3=0$

En cada hipoteis evaluada el valor p es menor a un nivel de significancia al 5% lo que indica que para cada caso, añadir el $\beta_i$ es significativamente distinto de 0

## Validación de supuestos

```{r, echo = T,fig.height=4}
validaciongrafica(model.lasso1)
```

```{r, echo = T}
hist(studres(model.lasso1),lwd=2,col='aquamarine4',
freq=F,ylim=c(0,0.4),xlab='Residuos Estudentizados Modelo',main='')
lines(density(studres(model.lasso1)),lwd=2,col='black')
```

Como se evidencia en los graficos y en las pruebas formales, ambos coinciden con sus respectivos resultados, es decir, varianza constante (Homocedasticidad) y distribución Normal en los errores.

## Identificación de puntosa atípicos e influyentes

Para esto utilizaremos la función influence.measures()

```{r, echo = T}
influence.measures(model.lasso1)$infmat[,-1]
#Puntos de Balanceo, Influyentes y Atípicos
par(mfrow=c(1,1))
influencePlot(model.lasso1,panel.first=grid(),ylab='Residuos Studentizados')
```

Dónde observamos que las observaciones 16,21 son influyentes a nuestro modelo y las 6,7 atípicas. Los puntos dentro de la base de datos lucen así y procedemos a ilustrarlos para que cuando un experto en el tema pueda considerarlos y evaluar si fueron errores de mediciones o que ocurre realmente con ellos. A su vez mostraremos los efectos de cada covariable con densidad. En general observamos que la relación de NIR2, 18 y 28 describen relaciones lineales negativas, inversamente proporcionales, a diferencia de la relación con NIR6 que es positiva, directamente proporcional.

```{r, echo=T}
X[c(6,7,16,21),c(2,6,18,28,31)]
plot(allEffects(model.lasso1))

```

Generamos una predicción con el modelo seleccionado:

```{r}
x.nuevo<- data.frame(NIR2=3.06,NIR6=2.55,NIR18=2.14,NIR28=1.32)
pred.media = predict(model.lasso1,x.nuevo,interval = "confidence")
pred.media

```

# Regresión ridge

A pesar de que evidenciamos claras mejoras en los problemas de multicolinealidad dada la selección de variables, procederemos a realizar la regresión de ridge que tiene como objetivo minimizar la siguiente suma de cuadrados penalizada:

```{=tex}
\begin{center}
$S_{k}(\beta)=\sum_{i=1}^{n}({y_{i}-z^
T}\beta)^2+k\sum_{j=1}^{p-1}\beta^2_{j}$
\end{center}
```
Luego de tener claro estos conceptos procedemos a realizar la regresión de ridge utilizando la librería lmridge. Para poder realizar esto generamos una secuencia de 1000 valores en los limites de 0 a 2, recordemos que el valor de párametro K es estrictamente positivo, generamos un proceso de validación obteniendo diversos criterios y al final seleccionamos un valor de k=0.1

```{r,echo=T}
# Regresión ridge
K = seq(from=0,to=2,length.out = 1000)
ridgedensity = lmridge(density~NIR2+NIR6+NIR18+NIR28,
                       data=X,K=K,scaling='sc')
#####
criterios<- kest(ridgedensity)
criterios
par(mfrow=c(1,2))
plot(K,criterios$GCV,panel.first=grid(),type='l',xlab='K',ylab='validación cruzada',main='GCV')
points(K[criterios$GCV==min(criterios$GCV)],
       criterios$GCV[criterios$GCV==min(criterios$GCV)],
       pch=19,col='red1')
text(K[criterios$GCV==min(criterios$GCV)],
     criterios$GCV[criterios$GCV==min(criterios$GCV)],
     labels=paste(K[1]),pos=3)
##########
plot(K,criterios$CV,panel.first=grid(),type='l',xlab='K',ylab='validación cruzada',main='CV')
points(K[criterios$CV==min(criterios$CV)],
       criterios$CV[criterios$CV==min(criterios$CV)],
       pch=19,col='red1')
text(K[criterios$CV==min(criterios$CV)],
     criterios$CV[criterios$CV==min(criterios$CV)],
     labels=paste(K[2]),pos=3)
###########
lambda<-c(K[criterios$GCV==min(criterios$GCV)],
          K[criterios$CV==min(criterios$CV)])
lambda
######
ridgedensity<-lmridge(density~NIR2+NIR6+NIR18+NIR28,
                      data=X,K=0.01,scaling='sc')
summary(ridgedensity)
```

### Interpretación

procedemos a realizar las interpretaciones del modelo por regresión de ridge donde evidenciamos cambios en los valores p de las pruebas individuales de los $\beta_{j}$, lo cual se debe a la disminución en varianza de las estimaciones. Observamos relaciones negativas entre la densidad y el conjunto de covariables NIR2, NIR 18 y NIR28 SI asumimos que se presentan aumentos con las demas covariables constantes, y se presenta una relación positiva entre la densidad y el NIR6 si mantenemos las demas covariables constantes. Es decir se mantienen las relaciones con el modelo principal, con algunas variaciones en las estimaciones. Ademas se cuenta con un $R^2_{adj}$ de 0.96440, el cual es $3\%$ menor a comparacion del principal.

```{r}
car::vif(model.lasso1)
lmridge::vif.lmridge(ridgedensity)

```

Observamos claras mejorar en los valores del VIF dónde a pesar de tener un par de valores por encima de 10, notamos claras mejoras, si aumentamos el valor de K, disminuira claramente estos factores de inflación de varianza pero aun así no es óptimo debido a que aumentamos el sesgo. Por último realizamos predicciones puntuales, dónde en R no había opciones para generar los intervalos de confianza por lo cual procedimos a realizar las estimaciones de la varianza de forma analítica para poder obtenerlos.

```{r}
#Para generar predicciones no es posible en una función en R
x.nuevo<- data.frame(NIR2=3.06,NIR6=2.55,NIR18=2.14,NIR28=1.32)
pred.media = predict(ridgedensity,x.nuevo,interval = "confidence")
pred.media
#Por lo cuál creamos la estimación de la varianza y matrix de covarianzas
# Obtener los coeficientes estimados y la matriz de diseño
beta_hat <- coef(ridgedensity)
X <- model.matrix(model.lasso1)

# Calcular la varianza del error
sigma2_hat <- sum(residuals(ridgedensity)^2) / (nrow(X)-length(coefficients(model.lasso1)))

# Calcular la matriz de covarianza de los coeficientes
V_beta_hat <- diag(residuals(ridgedensity)^2)*solve(t(X) %*% X
+0.01*diag(ncol(X))) %*% t(X) %*%X %*% solve(t(X) %*%
X + 0.01 * diag(ncol(X)))
# Hacer una predicción para nuevas observaciones
x.nuevo<- data.frame(NIR2=3.06,NIR6=2.55,NIR18=2.14,NIR28=1.32)
X_new <- cbind(1, as.matrix(x.nuevo))
pred <- X_new %*% beta_hat
se_pred <- sqrt(diag(X_new %*% V_beta_hat %*% t(X_new))) * sqrt(sigma2_hat)

# Calcular los intervalos de confianza del 95%
lower <- pred - qt(0.975, nrow(iris)-4) * se_pred
upper <- pred + qt(0.975, nrow(iris)-4) * se_pred

# Mostrar los resultados
data.frame(lower, pred, upper)

```

## Validación de supuestos regresión ridge

```{r}
X <- data.frame(matrix(c(yarn$NIR[,1:30],yarn$density),nrow =28, ncol= 31))
colnames(X) <- c(paste("NIR",1:30,sep=""),"density")
ridgedensity<-lmridge(density~NIR2+NIR6+NIR18+NIR28,
                      data=X,K=0.01,scaling='sc')
par(mfrow=c(1,2))
plot(fitted.values(ridgedensity),residuals(ridgedensity),pch=19,
     ylab='Residuos Ridge',xlab='Valores Ajustados',main='A',col="aquamarine4",
     ylim=c(-3,3))
abline(h=0,lwd=2,lty=2)
car::qqPlot(residuals(ridgedensity),xlab="Cuantiles Teóricos",ylab="Residuos Ridge",main="B",pch=19)
print('H0: Homocedasticidad vs H1: No hay homocedasticidad')
bptest(ridgedensity)
print('H0: Normalidad vs H1: No Normalidad')
shapiro.test(residuals(ridgedensity))
par(mfrow=c(1,1))
hist(residuals(ridgedensity),lwd=2,col='aquamarine4',
freq=F,ylim=c(0,0.4),xlab='Residuos Estudentizados Modelo',main='')
lines(density(residuals(ridgedensity)),lwd=2,col='black')
abline(h=0,lty=2,lwd=2)


```

Al evaluar los supuestos se observa que las pruebas formales afirman que los supuestos se cumplen, en cuanto a las pruebas graficas, en la normalidad de los resudios, se observan unos cuantos puntos que estan por fuera de las bandas de confianza, y en el grafico de los residuos se observa que se presenta una varianza constante en los errores.
