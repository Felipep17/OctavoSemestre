---
title: "Taller 2 Regresión lineal Multiple"
author: "Andrés Felipe Palomino - David Stiven Rojas"
date: "2023-04-21"
output: 
  pdf_document: 
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo =TRUE)
```

# Introducción

La base de datos $"yarn"$ obtenida de la librería (PLS) contiene información sobre espectros NIR y mediciones de densidad de hilos de PET, consta de 28 individuos (hilos de PET), 268 variables predictoras (NIRS) y una variable de respuesta (densidad). Se ajustará un modelo lineal múltiple para estimar la densidad del hilo PET, mediante mediciones NIR

```{r, echo = T, results = 'asis', message= FALSE, warning= FALSE}
#Importación de librerías necesarias
library(car)
library(glmnet)
library(MASS)
library(xtable)
library(lmtest)
library(readxl)
library(lmridge)
library(pls)
library(olsrr)
```

## Base de datos

En la siguiente tabla se encuentra un encabezado de la base de datos que se trabajara, esta consta de 30 covariables predictoras, las cuales estarán desde NIR1 hasta NIR30. De primera mano se observa que los valores de los NIR disminuyen a medida que la covariable aumenta

```{r, echo = T, results = 'asis', message= FALSE}

X <- data.frame(matrix(c(yarn$NIR[,1:30],yarn$density),nrow =28, ncol= 31))
colnames(X) <- c(paste("NIR",1:30,sep=""),"density")



```

## Funciones creadas

Antes de empezar con el proceso de seleccionar las variables para ajustar el modelo se crean funciones para optimizar el proceso de validación de supuestos, debido a que constantemente se deben realizar, estas funciones estan diseñadas para objetos lm.

```{r,echo = T, results = 'asis', message= FALSE}
##Validacion grafica para homocedasticidad y normalidad y pruebas formales
validaciongrafica<- function(model,cor=F){
  
  par(mfrow=c(1,2))
  plot(fitted.values(model),studres(model),panel.first=grid(),
       pch=19,ylab='Residuos Estudentizados',xlab='Valores ajustados',main='A',col='aquamarine4')
  abline(h=c(-2,0,2),lty=2)
  qqPlot(model,pch=19,ylab='Residuos Estudentizados',
         xlab='Cuantiles Teóricos',col=carPalette()[1],
         col.lines=carPalette()[3],main='B')
  print('Shapiro Test; H0: Normalidad vs H1: No Normalidad')
  print(shapiro.test(studres(model)))
  print('Breusch Pagan Test;H0: Homocedasticidad vs H1: No Homocedasticidad')
  print(bptest(model))
  if(cor==T){
    par(mfrow=c(1,2))
    plot(studres(model),type="b",xlab="Tiempo",ylab="Residuos Estudentizados",main="A",
         pch=19,panel.first=grid())
    plot(studres(model)[-length(fitted.values(model))],
         studres(model)[-1],pch=19,panel.first = grid(),col="turquoise3",
         xlab=TeX("$Residuos_{t-1}$"),ylab=TeX("$Residuos_{t}$"),main="B")
    abline(lm(studres(model)[-1]~studres(model)[-length(fitted.values(model))]))
    print('Durbin Watson Test')
    print(durbinWatsonTest(model,
                           method='resample',reps=10000))
  }
  par(mfrow=c(1,1))
}

## Calculo de lambda optimo para boxcox
lambda<- function(model,a,b){
  par(mfrow=c(1,1))
  box.cox<-boxcox(model,lambda=seq(a,b,length.out = 1000),
                  ylab='log-verosimilitud')
  bc<-round(box.cox$x[box.cox$y ==max(box.cox$y)],2)
  print(bc)
}

```

# Selección de variables

En el proceso de selección de variables se procede a realizar la Regresion de LASSO para identificar las posibles variables que tengan un aporte poco relevante, Por ultimo se ajustara el modelo cuyas variables tengan buenos indicadores y se pueda realizar corrección de supuestos

## Regresión de LASSO

Este es un método de regularización que se implementa cuando se tiene muchas covariables disponibles y se cree que pocas tienen un aporte relevante.

Se asume el modelo de regresión usual, donde :

```{=tex}
\begin{center}
E(y|x)=$X^T\beta$, y V(y|x)=$\sigma^2$
\end{center}
```
Donde se asume que algunos $\beta$ son cero. El objetivo del estimador es seleccionar los coeficientes que tienen valores diferentes de cero. El cual se obtiene minimizando la siguiente expresión:

```{=tex}
\begin{center}
$S_{lasso}(\beta)=\sum_{i=1}^{n}({y_{i}-x^
T}\beta)^2+\lambda\sum_{j=1}^{p-1}|\beta_{j}|$
\end{center}
```
Esta es la suma de cuadrados del estimador por MCO más una penalización ($\lambda$), a la suma del valor absoluto de los coeficientes. A medida que $\lambda$ aumenta la penalización tendrá mas peso sobre la estimación de los coeficientes, es decir que si la penalización es muy grande, todas las estimaciones seran cero. No hay solución analitica para $\hat{\beta}_{lasso}$ por lo que se usan algoritmos para la estimación, como lo es la funcion de glmnet de la libreria glmnet.

### Modelo a realizar regresión LASSO

Como se establecio anteriormente, se asume un modelo de regresión usual, el cual debe cumplir los siguientes supuestos: E(y\|x)=$x^T\beta$, y V(y\|x)=$\sigma^2$, es decir, varianza constante y E($\varepsilon$)=0 . Por ende es necesario proponer un modelo con p\<n, en el cual se eliminaran las variables con menor correlación con la variable density. Dicho modelo se expresa acontinuación y se evaluan los supuestos:

```{r, echo = T, results = 'asis', message= FALSE}

model <- lm(density ~ .-NIR1-NIR8-NIR9-NIR10-NIR11-NIR7, data=X)
car::vif(model)[1:5]
car::vif(model)[6:10]
car::vif(model)[11:16]
car::vif(model)[17:24]
validaciongrafica(model)
hist(studres(model),lwd=2,col='aquamarine4',freq=F,ylim=c(0,0.4),
     xlab='Residuos Estudentizados Modelo',main='')
lines(density(studres(model)),lwd=2,col='black')

```

Como no se cumple el supuesto de normalidad se procede a corregir mediante el metodo de BoxCox y se verifica el cumplimiento de los mismos.

```{r, echo = T, results = 'asis', message= FALSE}
model <- lm(density+0.01 ~ .-NIR1-NIR8-NIR9-NIR10-NIR11-NIR7, data=X)
lambda(model,0.5,1.5)
model.box <- lm(I(density^0.96) ~.-NIR1-NIR8-NIR9-NIR10-NIR11-NIR7,data=X)
car::vif(model.box)
validaciongrafica(model.box)
hist(studres(model.box),lwd=2,col='aquamarine4',
freq=F,ylim=c(0,0.4),xlab='Residuos Estudentizados Modelo',main='')
lines(density(studres(model.box)),lwd=2,col='black')


```

Ya con los requerimentos necesarios para realizar regresión de LASSO, se procede a calcular las estimaciones para distintos valores de $\lambda$ que se muestran en la siguiente figura:

```{r, echo = T, results = 'asis', message= FALSE}

X.<-model.matrix(model.box)[,-1]
lasso.mod <- glmnet(X., X$density, alpha = 1,nlambda = 100)
plot(lasso.mod,xvar='lambda',label=T,lwd=2,ylab='coeficientes de regresión')
abline(h=0,lty=2)


```

Para identificar el valor de $\lambda$ optimo se procede a realizar validación cruzada.

### Validación cruzada

Es un método para evaluar que tan bueno es un modelo para predecir observaciones futuras de la población objeto de estudio. La muestra se divide en dos grupos:

-   Entrenamiento: Se usa para ajustar el modelo.

-   Validación: Se utiliza para validar el modelo ajustado.

```{r, echo = T, }

lasso.cv <-cv.glmnet(X., X$density, nfolds = 4, alpha = 1,
                     nlambda = 100)
plot(lasso.cv)
est = glmnet(X., X$density, alpha = 1,lambda = lasso.cv$lambda.1se)
est$beta

```

La selección de variables por medio del estimador LASSO son: NIR2, NIR6, NIR18, NIR28, NIR29. Consiguiente a eso se procede a realizar una suma extra de cuadrados para evaluar si podemos eliminar NIR29 para disminuir problemas de multicolinealidad.

## Suma extra de cuadrados

Sirve para probar la significancia de un subconjunto de coeficientes.

Se tiene el siguiente modelo:

$$y = X\beta+\varepsilon$$ donde $\beta$=$\begin{bmatrix} \beta_{1}\\ \beta_{2} \\ \end{bmatrix}$

donde $\beta_1$ es un vector (p-r)x1 y $\beta_2$ es un vector rx1 , se quiere evaluar la siguiente hipotesis:

```{=tex}
\begin{center}
$H_0 : \beta_2 =0$ vs $H_1: \beta_2 \neq 0 $
\end{center}
```
Se tienen los siguientes modelos: Modelo completo : $y = X_1\beta_1+X_2\beta_2+\varepsilon$ Modelo reducido : $y = X_1\beta_1\varepsilon$

```{r, echo = T}
model.lasso1 <- lm(density~NIR2+NIR6+NIR18+NIR28+NIR29,data=X)
model.lasso2 <- lm(density~NIR2+NIR6+NIR18+NIR28,data=X)
anova(model.lasso2,model.lasso1)
car::vif(model.lasso1)
car::vif(model.lasso2)
```

# Modelo de regresión multiple

Con base en el proceso de selección de variables se ajusta el siguiente modelo y se realiza la respectiva validación de supuestos:

```{r, echo = T}
model.lasso1<- lm(density~NIR2+NIR6+NIR18+NIR28,data=X)
summary(model.lasso1)
car::vif(model.lasso1)
```

```{r, echo = T}
validaciongrafica(model.lasso1)
car::vif(model.lasso1)
hist(studres(model.lasso1),lwd=2,col='aquamarine4',
freq=F,ylim=c(0,0.4),xlab='Residuos Estudentizados Modelo',main='')
lines(density(studres(model.lasso1)),lwd=2,col='black')
```

## Identificación de puntosa atípicos e influyentes

Para esto utilizaremos la función influence.measures()

```{r, echo = T}
influence.measures(model.lasso1)$infmat[,-1]
#Puntos de Balanceo, Influyentes y Atípicos
par(mfrow=c(1,1))
influencePlot(model.lasso1,panel.first=grid(),ylab='Residuos Studentizados')
```

Dónde observamos que las observaciones 16,21son influyentes a nuestro modelo y las 6,7 atípicas. Los puntos dentro de la base de datos lucen así y procedemos a ilustrarlos para que cuando un experto en el tema pueda considerarlos y evaluar si fueron errores de mediciones o que ocurre realmente con ellos.

```{r, echo=T}
X[c(6,7,16,21),c(2,6,18,28,31)]


```

A pesar de que evidenciamos claras mejoras en los problemas de multicolinealidad dada la selección de variables, procederemos a realizar la regresión de ridge que propone la siguiente estimación:

```{r,echo=T}
# Regresión ridge
K = seq(from=0,to=2,length.out = 1000)
ridgedensity = lmridge(density~NIR2+NIR6+NIR18+NIR28,
                       data=X,K=K,scaling='sc')
#####
criterios<- kest(ridgedensity)
criterios
par(mfrow=c(1,2))
plot(K,criterios$GCV,panel.first=grid(),type='l',xlab='K',ylab='validación cruzada',main='GCV')
points(K[criterios$GCV==min(criterios$GCV)],
       criterios$GCV[criterios$GCV==min(criterios$GCV)],
       pch=19,col='red1')
text(K[criterios$GCV==min(criterios$GCV)],
     criterios$GCV[criterios$GCV==min(criterios$GCV)],
     labels=paste(K[1]),pos=3)
##########
plot(K,criterios$CV,panel.first=grid(),type='l',xlab='K',ylab='validación cruzada',main='CV')
points(K[criterios$CV==min(criterios$CV)],
       criterios$CV[criterios$CV==min(criterios$CV)],
       pch=19,col='red1')
text(K[criterios$CV==min(criterios$CV)],
     criterios$CV[criterios$CV==min(criterios$CV)],
     labels=paste(K[2]),pos=3)
###########
lambda<-c(K[criterios$GCV==min(criterios$GCV)],
          K[criterios$CV==min(criterios$CV)])
lambda
######
ridgedensity<-lmridge(density~NIR2+NIR6+NIR18+NIR28,
                      data=X,K=0.01,scaling='sc')
summary(ridgedensity)
vif.lmridge(ridgedensity)
car::vif(model.lasso1)
lmridge::vif.lmridge(ridgedensity)

par(mfrow=c(1,2))
plot(fitted.values(ridgedensity),residuals(ridgedensity),pch=19,
     ylab='Residuos Ridge',xlab='Valores Ajustados',main='A',col="aquamarine4",
     ylim=c(-3,3))
abline(h=0,lwd=2,lty=2)
car::qqPlot(residuals(ridgedensity),xlab="Cuantiles Teóricos",ylab="Residuos Ridge",main="B",pch=19)
print('H0: Homocedasticidad vs H1: No hay homocedasticidad')
bptest(ridgedensity)
print('H0: Normalidad vs H1: No Normalidad')
shapiro.test(residuals(ridgedensity))
par(mfrow=c(1,1))
hist(residuals(ridgedensity),lwd=2,col='aquamarine4',
freq=F,ylim=c(0,0.4),xlab='Residuos Estudentizados Modelo',main='')
lines(density(residuals(ridgedensity)),lwd=2,col='black')
abline(h=0,lty=2,lwd=2)


```
